{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1Q4DtpcR7m2jQPe4zKIJ59irV0kDJpiwr","authorship_tag":"ABX9TyNtYj6lsPysSP//wTr90+Mm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"e9dd91bd67a64fcca2950d45ce8a036a":{"model_module":"@jupyter-widgets/controls","model_name":"BoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"BoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"BoxView","box_style":"","children":["IPY_MODEL_bafbab0d8e3b4095a91ffb5b73e060b6"],"layout":"IPY_MODEL_08191eea271d418f8c49f9b8cda9e9e1"}},"bafbab0d8e3b4095a91ffb5b73e060b6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5065bfeacd6641c784e9fd93143f9303","IPY_MODEL_d4cfa0c4db234014a0a6a105fff1c0c4","IPY_MODEL_f995e8b55dfd45ca97a2658dd0b70d46"],"layout":"IPY_MODEL_4214de2be88a433a9ecdc3bac651258c"}},"08191eea271d418f8c49f9b8cda9e9e1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":"center","justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5065bfeacd6641c784e9fd93143f9303":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":["custom-button"],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"1. Load Data","disabled":false,"icon":"","layout":"IPY_MODEL_f30dfa66ee5c402092189d6240394195","style":"IPY_MODEL_9a42f6241baf4601a55d340198d3c3e3","tooltip":""}},"d4cfa0c4db234014a0a6a105fff1c0c4":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":["custom-button"],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"2. EDA Analysis","disabled":false,"icon":"","layout":"IPY_MODEL_2e68bccd85cf4cc48e0fd5757797b467","style":"IPY_MODEL_a1d45c4171714ed4ba2e580dc3de2d76","tooltip":""}},"f995e8b55dfd45ca97a2658dd0b70d46":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":["custom-button"],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"3. Pre Process Data","disabled":false,"icon":"","layout":"IPY_MODEL_06c93b7633554e65ac4cb503e6953fbb","style":"IPY_MODEL_3d6e8185bc7a42cb93fd2e2c06093ee6","tooltip":""}},"4214de2be88a433a9ecdc3bac651258c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f30dfa66ee5c402092189d6240394195":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9a42f6241baf4601a55d340198d3c3e3":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"2e68bccd85cf4cc48e0fd5757797b467":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a1d45c4171714ed4ba2e580dc3de2d76":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"06c93b7633554e65ac4cb503e6953fbb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3d6e8185bc7a42cb93fd2e2c06093ee6":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"54d3069ea7e142ab9d5925f924916e31":{"model_module":"@jupyter-widgets/controls","model_name":"BoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"BoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"BoxView","box_style":"","children":["IPY_MODEL_8fd3e07129524dd39740aa2b9ccd6011"],"layout":"IPY_MODEL_08191eea271d418f8c49f9b8cda9e9e1"}},"8fd3e07129524dd39740aa2b9ccd6011":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c9fea9baf2f846a392a5736127ccdc1e","IPY_MODEL_aeba101823e642f4b87a2b7ade7a4480","IPY_MODEL_704bd9b1b00a41fd9358c39ee14b733d"],"layout":"IPY_MODEL_e9d432fd493a479abe87a486e45feb13"}},"c9fea9baf2f846a392a5736127ccdc1e":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":["custom-button"],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"4. Transform & Build","disabled":false,"icon":"","layout":"IPY_MODEL_3764bebee3bc41e39484267bc4e5cf20","style":"IPY_MODEL_59a103bc21fb43ccaea852ac4ced1675","tooltip":""}},"aeba101823e642f4b87a2b7ade7a4480":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":["custom-button"],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"5. Prediction(Bulk Mails)","disabled":false,"icon":"","layout":"IPY_MODEL_3fd06e4a16384a2da18f1e5952ce1324","style":"IPY_MODEL_f4b403023c654be59547e87e0dece506","tooltip":""}},"704bd9b1b00a41fd9358c39ee14b733d":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":["custom-button"],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"6. Prediction(Single Mail)","disabled":false,"icon":"","layout":"IPY_MODEL_e20de2acffd1403090ea41fc19f1d9a1","style":"IPY_MODEL_f2d245be25274337bcabcc7ec9560d75","tooltip":""}},"e9d432fd493a479abe87a486e45feb13":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3764bebee3bc41e39484267bc4e5cf20":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"59a103bc21fb43ccaea852ac4ced1675":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"3fd06e4a16384a2da18f1e5952ce1324":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f4b403023c654be59547e87e0dece506":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"e20de2acffd1403090ea41fc19f1d9a1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f2d245be25274337bcabcc7ec9560d75":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":342},"id":"iVV6xtAEhH4T","executionInfo":{"status":"ok","timestamp":1700679521349,"user_tz":-330,"elapsed":77421,"user":{"displayName":"TANUSHREE MONDAL HAZRA","userId":"12079523468331201661"}},"outputId":"e7f5f766-aa5e-4926-aa15-17efa8d459be"},"outputs":[{"output_type":"stream","name":"stdout","text":["mv: cannot stat '/content/drive/MyDrive/Colab Notebooks/spam_detection_SVM.ipynb': No such file or directory\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","        document.getElementById('output-area').scrollIntoView({behavior: 'smooth'});\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Collecting colorama\n","  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n","Installing collected packages: colorama\n","Successfully installed colorama-0.4.6\n","Collecting pyspark\n","  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n","Building wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425344 sha256=2d2cb4bd685a6ea79e40d5d7152ef18a4f6bfef8ecc8f2f67937186e7afd36d7\n","  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n","Successfully built pyspark\n","Installing collected packages: pyspark\n","Successfully installed pyspark-3.5.0\n"]}],"source":["######### Insert the following in CODE CELL 1\n","\n","####################################\n","# This program is same as that defined in 'spam_not_spam_SVM.ipynb' the only\n"," # Difference is this program execute all functions using Button Events\n","#####################################\n","\n","# spam detection in emails - Core Algorithm used is SVM, Accuracy above 97 %\n","#########################################################################\n","\n","# Run the following commands only once, after mounting and connect\n","# Change Untitled.ipynb name to your acutal program name, then run the\n","# following.\n","\n","#### Accuracy of SVM is 98 %, Notebook name is 'spam_not_spam_SVM', select this\n","#### Accuracy of NAIVEBAYES is 90 %\n","#### Accuracy of RANDOM FOREST is below 70 %\n","\n","########### Start your program from here\n","\n","# Why the !mv (Move Command)\n","# If you want to connect your actual program using another login program you should\n","# save all your programs in \"/content/drive/MyDrive\", folder\n","\n","# By default colab store your program in folder '/content/drive/MyDrive/Colab Notebooks/'\n","# !mv command copies your program from the above folder to \"/content/drive/MyDrive\"\n","\n","!mv \"/content/drive/MyDrive/Colab Notebooks/spam_detection_SVM.ipynb\" \"/content/drive/MyDrive\"\n","\n","# Define function to set focus to result output grid, other wise you will have to manually\n","# scroll down to bottom of this cell to view results\n","\n","from IPython.display import display, HTML,Javascript\n","\n","def set_focus():\n","    display(Javascript(\"\"\"\n","        document.getElementById('output-area').scrollIntoView({behavior: 'smooth'});\n","    \"\"\"))\n","\n","# Set focus to result grid\n","set_focus()\n","\n","# Import color palettes\n","\n","!pip install colorama\n","from colorama import Fore, Style, Back\n","import tqdm\n","\n","# Install all necessary packages and libraries\n","\n","!pip install pyspark\n","import pyspark\n","\n","from matplotlib import pyplot as plt\n","from pyspark.ml.classification import LinearSVCModel\n","from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n","from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF, StringIndexer, \\\n","     VectorAssembler\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import substring, length, explode, col, when\n","from pyspark.ml.classification import LinearSVC\n","\n","# Import the necessary libraries for user inputs\n","\n","import ipywidgets as VBox\n","from IPython.display import display, HTML\n","from traitlets import TraitError\n","import csv\n","import ipywidgets as widgets\n","from IPython.display import display, HTML,Javascript\n","\n","# Clear Result Grid package\n","\n","from IPython.display import clear_output\n","\n","# Create an instance of SparkSession, and store it in 'my_spark'. You can use any name\n","# other than 'my_spark'\n","\n","my_spark = SparkSession.builder.getOrCreate()"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"7unzGt9URK33","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700679571891,"user_tz":-330,"elapsed":13221,"user":{"displayName":"TANUSHREE MONDAL HAZRA","userId":"12079523468331201661"}},"outputId":"f9a92e7f-2cbb-4e9f-cdd1-1ceb20caa3f5"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["######## Insert the following in CODE CELL 2\n","\n","######################\n","# Important : To view output in Full Screen without showing codes,\n","# R.click Code cell-->'View output fullscreen'\n","#######################\n","\n","# @title\n","\n","# Set focus to result grid\n","set_focus()\n","\n","\n","# Function to clear the result grid\n","\n","def clear_result_grid():\n","    clear_output(wait=True)\n","\n","def load_data(event):\n","    print()\n","    clear_result_grid()\n","# global command is used to access df_from_csv dataframe in other functions,\n","# otherwise df_from_csv not declared error will flag by python\n","    global df_from_csv\n","    print()\n","    print()\n","\n","    display(HTML(\"<h3 style='color:orange; text-align:left;'>Loading Data, this may take few seconds...wait !!!</h3>\"))\n","\n","    print()\n","    print()\n","# The .option(\"quote\", \"\\\"\") specifies some email messages are inside quotation marks, so\n","# remove quotes and read it as a single mail\n","\n","    df_from_csv = my_spark.read \\\n","        .format(\"csv\") \\\n","        .option(\"header\", True) \\\n","        .option(\"inferSchema\", True) \\\n","        .option(\"quote\", \"\\\"\") \\\n","        .load(\"/content/drive/MyDrive/spam_naipu.csv\")\n","\n","    display(HTML(\"<h3 style='color:magenta; text-align:left;'>All your Data loaded Successfully...</h3>\"))\n","    main_function()\n","def EDA_start(event):\n","    print()\n","    print()\n","#    clear_result_grid()\n","    display(HTML(\"<h3 style='color:orange; text-align:left;'>EDA Started, Please wait !!!</h3>\"))\n","\n","# Declare global df_from_csv again otherwise unbound error will be displayed in\n","# command, df_from_csv = df_from_csv.dropna(subset=['text'])\n","\n","    global df_from_csv\n","\n","    print(\"Schema of emails_dataset\")\n","    df_from_csv.printSchema()\n","\n","    print(\"Data in emails_dataset\")\n","    df_from_csv.show(2)\n","\n","# Display count of total 'ham' and 'spam' messages as follows:-\n","\n","    tot_records = df_from_csv.count()\n","    ham_count = df_from_csv.filter(col('class') == 'ham').count()\n","    spam_count = df_from_csv.filter(col('class') == 'spam').count()\n","\n","    not_ham_spam_count = df_from_csv.filter((col('class') != 'ham') &\n","     (col('class') != 'spam')).count()\n","\n","    print(\"Total mails =\", tot_records)\n","    print(\"Total ham mails =\", ham_count)\n","    print(\"Total spam mails =\", spam_count)\n","\n","# Check for null values\n","\n","    null_count = df_from_csv.filter(col(\"text\").isNull()).count()\n","    print(\"Null values =\", null_count)\n","\n","    print(\"Total erroneous mails including nulls =\", not_ham_spam_count)\n","\n","# Findings :\n","#~~~~~~~~~~\n","# Total mails      = 5574\n","# Total ham mails  = 4825\n","# Total spam mails =  747\n","\n","# Null values = 1\n","# Total erroneous mails including nulls = 2 (class column contain\n","# other than 'ham' or 'spam' category)\n","\n","# Remove all erroneous rows, in this case 2 rows\n","# The 2 rows are removed in pre_process_data function below\n","\n","###### Display average lengths of 'ham' and 'spam' messages\n","\n","# To get avg, create a view and use simple SQL statement as follows\n","\n","    df_from_csv.createOrReplaceTempView(\"avg_view\")\n","\n","# First group all 'ham' messages with Average, store it in avg-ham\n","\n","    ham_msgs = my_spark.sql(\"SELECT AVG(LENGTH(text)) AS avg_ham FROM avg_view WHERE class = 'ham'\")\n","    spam_msgs = my_spark.sql(\"SELECT AVG(LENGTH(text)) AS avg_spam FROM avg_view WHERE class = 'spam'\")\n","\n","    sum_ham_msgs = my_spark.sql(\"SELECT SUM(LENGTH(text)) AS sum_ham FROM avg_view WHERE class = 'ham'\")\n","    sum_spam_msgs = my_spark.sql(\"SELECT SUM(LENGTH(text)) AS sum_spam FROM avg_view WHERE class = 'spam'\")\n","\n","    sum_ham_msgs_converted = round(sum_ham_msgs.first().sum_ham, 2)\n","    sum_spam_msgs_converted = round(sum_spam_msgs.first().sum_spam, 2)\n","\n","    print(\"Total Characters in ham messages =\", sum_ham_msgs_converted)\n","    print(\"Total Characters in spam messages =\", sum_spam_msgs_converted)\n","\n","# Retrieve average, show result using 2 decimal places\n","\n","    ham_msg_length  = round(ham_msgs.first().avg_ham, 2)\n","    spam_msg_length = round(spam_msgs.first().avg_spam, 2)\n","\n","    print(\"Average Length of ham message=\", ham_msg_length)\n","    print(\"Average Length of spam message=\", spam_msg_length)\n","\n","# Average Length of ham message= 71.07 Characters\n","# Average Length of spam message= 138.46 Characters\n","\n","# Findings : The above 'ham','spam' message lengths clearly idicates that, in the given\n","# ~~~~~~~~~~\n","# Dataset, spam messages are longer than ham messages. So include these lengths as a feature\n","# while train your model.\n","\n","    display(HTML(\"<h3 style='color:magenta; text-align:left;'>EDA Operations finished...</h3>\"))\n","    main_function()\n","def pre_process_data(event):\n","    print()\n","    clear_result_grid()\n","    display(HTML(\"<h3 style='color:orange; text-align:left;'>Pre-Processing of Data Started, Please wait !!!</h3>\"))\n","\n","# Declare 'global df_from_csv' again otherwise unbound error will be displayed in\n","# command, df_from_csv = df_from_csv.dropna(subset=['text'])\n","\n","    global df_from_csv\n","\n","#  Preprocess Stage 1: Remove all Null values from 'class' and 'text' columns\n","\n","    df_from_csv = df_from_csv.dropna(subset=['class'])\n","    df_from_csv = df_from_csv.dropna(subset=['text'])\n","\n","# Cross verify after removing null values\n","\n","    null_count = df_from_csv.filter(col(\"text\").isNull()).count()\n","\n","    print(\"Null values =\", null_count)\n","\n","# Null values = 0, One null value found and removed\n","\n","# Remove other erroneous rows\n","\n","# Filter Only 'ham' and 'spam' form 'class' column of df_from_csv dataset\n","\n","    df_from_csv = df_from_csv.filter((col('class') == 'ham') |\n","     (col('class') == 'spam'))\n","\n","    print(\"Cross verify the filtered df_from_csv Dataset\")\n","    df_from_csv.describe().show()\n","\n","# The following count of class and text column shows correct number of emails as 5572\n","\n","# +-------+-----+--------------------+\n","# |summary|class|                text|\n","# +-------+-----+--------------------+\n","# |  count| 5572|                5572|\n","# |   mean| null|               645.0|\n","# | stddev| null|                null|\n","# |    min|  ham| &lt;#&gt;  in mc...|\n","# |    max| spam|  _ we r stayin h..|\n","# +-------+-----+--------------------+\n","\n","\n","# Calculate Text Length of each mail and add it along with the original dataset\n","# text length plays a key role in predicting 'ham' or 'spam' mails. The reason is that,\n","# normaly spam messages contains more number of characters than ham mails\n","\n","    global add_length_new\n","\n","    add_length_col = df_from_csv.withColumn(\"text_length\",\n","                                                length(df_from_csv[\"text\"]))\n","\n","    print(\"Schema of add_length_col Dataset...\")\n","    add_length_col.printSchema()\n","# root\n","#  |-- class: string (nullable = true)\n","#  |-- text: string (nullable = true)\n","#  |-- text_length: integer (nullable = true) New column added\n","\n","# Cross verify data in 'add_length_col' dataset\n","\n","    print(\"Data in add_length_col DF...\")\n","    add_length_col.show(5)\n","\n","# +-----+--------------------+-----------+\n","# |class|                text|text_length|\n","# +-----+--------------------+-----------+\n","# | spam|XXXMobileMovieClu...|        149|\n","# |  ham|Oh k...i'm watchi...|         26|\n","# |  ham|Eh u remember how...|         81|\n","# |  ham|Fine if that  s t..|         58|\n","# | spam|England v Macedon...|        155| Spam Messages shows high text lengths\n","# +-----+--------------------+-----------+\n","\n","# Remove zero length documents\n","\n","    add_length_col.createOrReplaceTempView(\"drop_zero_docs\")\n","\n","    remove_0_docs = my_spark.sql(\"SELECT * FROM drop_zero_docs where text_length > 0\")\n","\n","    remove_0_docs.createOrReplaceTempView(\"clean_class_view\")\n","\n","    add_length_new = my_spark.sql(\"SELECT * FROM clean_class_view WHERE class = 'ham' OR class = 'spam'\")\n","\n","    print(\"Class column cleaned add_length_new\")\n","    add_length_new.describe().show()\n","\n","    print(\"Average lenths of ham and spam messages are....\")\n","    add_length_new.groupby(\"class\").mean().show()\n","# +-----+------------------+\n","# |class|  avg(text_length)|\n","# +-----+------------------+\n","# |  ham|  71.0334510700187|\n","# | spam|138.31935047361299|\n","# +-----+------------------+\n","\n","    print(\"Schema of add_length_new\")\n","    add_length_new.printSchema()\n","\n","# root\n","#  |-- class: string (nullable = true)\n","#  |-- text: string (nullable = true)\n","#  |-- text_length: integer (nullable = true) this column is added in actual dataset\n","\n","    display(HTML(\"<h3 style='color:magenta; text-align:left;'>Pre-Processing Finished...</h3>\"))\n","    main_function()\n","def transformation(event):\n","    print()\n","    set_focus()\n","    clear_result_grid()\n","    display(HTML(\"<h3 style='color:orange; text-align:left;'> \\\n","      Transformation Started may take less than 2 Minutes, Please wait !!!</h3>\"))\n","\n","# Create an Instance of Tokenizer as tokenizer, to split each words separately\n","\n","    tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"text_tokenized\")\n","\n","# Apply Tokenizer into add_length_col DF, to get each word separately\n","\n","    tokenized_words_df = tokenizer.transform(add_length_new)\n","\n","    print(\"Schema of tokenized_words_df dataframe...\")\n","    tokenized_words_df.printSchema()\n","# root\n","#  |-- class: string (nullable = true)\n","#  |-- text: string (nullable = true)\n","#  |-- text_length: integer (nullable = true)\n","#  |-- text_tokenized: array (nullable = true)  new column added, contains each word\n","#  |    |-- element: string (containsNull = true)\n","\n","    print(\"Display tokenized_words_df dataframe...\")\n","    tokenized_words_df.show(5, False, vertical=True)\n","# -RECORD 0------------------------------------------------------------------------------\n","#  class          | spam\n","#  text           | XXXMobileMovieClub: To use your credit, click the WAP link in the next...\n","#  text_length    | 149\n","#  text_tokenized | [xxxmobilemovieclub:, to, use, your, credit,, click, the, wap, link, in...\n","\n","# Remove Stop words as follows, 'is','you','i','and','we'..etc:-\n","\n","    stop_words_grouping = StopWordsRemover(inputCol=\"text_tokenized\", outputCol=\"text_with_no_stopwords\")\n","\n","#    stop_words_removed = stop_words_grouping.transform(tokenizer.transform(add_length_col))\n","\n","    stop_words_removed = stop_words_grouping.transform(tokenized_words_df)\n","\n","    print(\"Stop words removed DF, stop_words_removed...\")\n","    stop_words_removed.show(5, False, vertical=True)\n","\n","# Count total number of indivudual words in the whole datast\n","\n","    stop_words_exploded = stop_words_removed.select(explode(\"text_with_no_stopwords\"))\n","    total_words = stop_words_exploded.count()\n","    print(\"Total number of words after removing Stop words:\", total_words)\n","\n","\n","    total_words_before = stop_words_removed.select(explode(\"text_tokenized\"))\n","    total_words_before_SWR = total_words_before.count()\n","\n","    print(\"Total number of words in the whole Document:\", total_words_before_SWR)\n","\n","# Total number of words in the whole Document: 86348\n","\n","# In PySpark, CountVectorizer is a machine learning feature extraction tool that is\n","# used to convert a collection of text documents into a matrix of token counts.\n","# The CountVectorizer function takes a sequence of text documents as input and\n","# produces a sparse matrix of token counts as output.\n","\n","# In other words CountVectorizer gives a value to each words in a single document\n","# that is, if 'A=seb is teaching java' , seb = 1 / 4, seb appears only once in\n","# document A which contains 4 words\n","\n","# A sparse matrix is a matrix in which most of the elements are zero. In other words,\n","# it is a matrix that has a large number of zero elements compared to the total number of elements.\n","\n","# Important : The parameter 'vocabSize' is mandatory while declaring CountVectorizer, otherwise when\n","# you give user emails into your model will throw 'stage failure' and 'Index out of range' errors\n","\n","# How to determine the vocabSize ? Take 10 percent of total words in the whole document,\n","# In the origianl dataset nearly 86,000 words are found. But the distinct words will\n","# come around 80,0000. So 10 % of 80,000 will be 8000\n","\n","    count_vector = CountVectorizer(inputCol='text_tokenized',\n","                                   outputCol='count_vectors', vocabSize=8000)\n","\n","# fit the CountVectorizer to the data\n","\n","    my_model = count_vector.fit(tokenized_words_df)\n","\n","# transform the data using the CountVectorizer\n","\n","    result = my_model.transform(tokenized_words_df)\n","\n","    print(\"Data In result DF, after count vector function is applied...\")\n","    result.show(5, False, vertical=True)\n","\n","# How to Interpret the result DF, count_vectors colmn\n","\n","# The first element of the tuple, 8000, represents the size of the vector, or total features\n","# i.e., the total number of elements in the vector.\n","\n","# The second element of the tuple, [0,4,8,11,24,81,188,198,268,1202,1379,1400,7273],\n","# represents the indices of the non-zero elements in the vector. In this case, the non-zero\n","# elements are at positions [0,4,8,11,24,81,188.....] etc\n","\n","# -RECORD 0------------------------------------------------------------------------------------------\n","#  class          | spam\n","#  text           | XXXMobileMovieClub: To use your credit, click the WAP link in the next txt .....\n","#  text_length    | 149\n","#  text_tokenized | [xxxmobilemovieclub:, to, use, your, credit,, click, the, wap, link, in, the,...\n","#  count_vectors  | (8000,[0,4,8,11,24,81,188,198,268,1202,1379,1400,7273],[1.0,2.0,1.0,1.0,1.......\n","#\n","\n","# Calculate Inverse document frequency of words(IDF)\n","#  IDF is used to determine the importance of a term (word) in a corpus (collection of documents)\n","# that is if 'A=seb is teaching java', 'B = seb is teaching spark'\n","# seb = log(2 / 2). Here we have a corpus contains 2 documents\n","# so 'seb' appears only twice in 2 documents(corpus)\n","\n","    tf_idf = IDF(inputCol=\"count_vectors\", outputCol=\"tf_idf\")\n","\n","    tf_idf_model = tf_idf.fit(result)\n","\n","# Transform tf_idf_model with original data, here in tf_idf_transformed dataframe\n","# the .transform(reuslt), tf and idf values are multiplied together\n","\n","    tf_idf_transformed = tf_idf_model.transform(result)\n","\n","    print(\"transformed tf_idf_model...\")\n","\n","    tf_idf_transformed.printSchema()\n","\n","    inputs = [\"tf_idf\", \"text_length\"]\n","    output = \"features\"\n","\n","    ham_spam_assembler = VectorAssembler(inputCols= inputs , outputCol= output)\n","\n","# Create Vectorized Features using tf-Idf dataframe. tf_idf is transformed from tokenized_words\n","# data frame\n","\n","    feature_df = ham_spam_assembler.transform(tf_idf_transformed)\n","\n","# Cross Verify\n","\n","    print(\"Schema of feature_df\")\n","    feature_df.printSchema()\n","\n","# Below schemat tf_idf column contains products of term frequency and inverse document frequencies\n","# in vector format\n","\n","# features column contains tf * idf values along with text length of each document\n","\n","# root\n","#  |-- class: string (nullable = true)\n","#  |-- text: string (nullable = true)\n","#  |-- text_length: integer (nullable = true)\n","#  |-- text_tokenized: array (nullable = true)\n","#  |    |-- element: string (containsNull = true)\n","#  |-- count_vectors: vector (nullable = true)\n","#  |-- tf_idf: vector (nullable = true)\n","#  |-- features: vector (nullable = true)\n","\n","# Data in feature_df\n","\n","    print(\"Data in feature_df\")\n","    feature_df.show(5, False, vertical=True)\n","\n","# Finding :  features column shows 8001 vectors, because along with tf_idf dataframe\n","# text_length is also added\n","\n","# -RECORD 0------------------------------------------------------------------------------------------\n","# class          | spam\n","# text           | XXXMobileMovieClub: To use your credit, click the WAP link in the next txt message ...\n","# text_length    | 149\n","# text_tokenized | [xxxmobilemovieclub:, to, use, your, credit,, click, the, wap, link, in, the, next....\n","# count_vectors  | (8000,[0,4,8,11,24,81,188,198,268,1202,1379,1400,7273],[1.0,2.0,1.0,1.0,1.0,1.0,1.0...\n","# tf_idf         | (8000,[0,4,8,11,24,81,188,198,268,1202,1379,1400,7273],[1.2069184916283124,......)\n","# features       | (8001,[0,4,8,11,24,81,188,198,268,1202,1379,1400,7273,8000.......,149]\n","\n","# Create an indexer for class column to Add label vector\n","\n","    ham_spam_indexer = StringIndexer(inputCol='class', outputCol='label')\n","\n","# Add the above label column with feature_df Dataframe\n","\n","    label_df = ham_spam_indexer.fit(feature_df).transform(feature_df)\n","\n","    print(\"Schema of label-df....\")\n","    label_df.printSchema()\n","\n","# root\n","#  |-- class: string (nullable = true)\n","#  |-- text: string (nullable = true)\n","#  |-- text_length: integer (nullable = true)\n","#  |-- text_tokenized: array (nullable = true)\n","#  |    |-- element: string (containsNull = true)\n","#  |-- count_vectors: vector (nullable = true)\n","#  |-- tf_idf: vector (nullable = true)\n","#  |-- features: vector (nullable = true)\n","#  |-- label: double (nullable = false)\n","\n","    print(\"Display data in label_df..\")\n","    label_df.show(5, False, vertical=True)\n","\n","#     ############### Split Training and Test Data begins here ##################\n","\n","# Cross Verify : count total number of documents or sentences in label_df as follows:-\n","\n","    total_docs = label_df.count()\n","    print(\"Total number of documents...\", total_docs)\n","\n","# Total number of documents... 5552, Tallied with Total Rows after cleaning\n","\n","# Split 70 % for training and 30 % for testing\n","\n","    train_docs = int(total_docs * 0.70)\n","\n","    test_docs = total_docs - train_docs\n","\n","# Cross Verify train_docs and test_docs count\n","\n","    print(\"Total Train documents = \", train_docs)\n","    print(\"Total Test documents = \", test_docs)\n","\n","# Cross Verify\n","\n","# Total number of documents... 5552\n","# Total Train documents =  3886\n","# Total Test documents =  1666\n","\n","###### Findings : If Random split not showing correct results then manually extract\n","# training data and test data as follows. Otherwise you can proceed with Random split\n","######################\n","\n","#     Extract 2935 documents or rows from label_df\n","#\n","#     train_data_df = label_df.limit(train_docs)\n","#\n","#     Similarly extract the remaining 1259 rows from label_df, for testing\n","#\n","#     test_data_df = label_df.subtract(train_data_df).limit(test_docs)\n","\n","############ random split starts here\n","\n","    train_data_df, test_data_df = label_df.randomSplit([0.7, 0.3], seed=5043)\n","\n","    print(\"Schema of test_data_df\")\n","    test_data_df.printSchema()\n","\n","    print(\"Data in Test Data DF\")\n","    test_data_df.show(2, False, vertical=True)\n","\n","\n","################# Create an SVM model object starts here ##########\n","\n","# maxIter is, A maxIter value of 10 means that the algorithm will perform up to 10\n","# iterations to optimize the model. If the algorithm converges before reaching the maximum\n","# number of iterations, it will stop early. If the algorithm has not converged after maxIter\n","# iterations, it will stop and return the current best solution.\n","\n","# regmParam=0.1, In PySpark's implementation of Support Vector Machines (SVMs), the regParam parameter\n","# is a regularization parameter that controls the amount of regularization applied to the model.\n","# Regularization is a technique used to prevent overfitting by adding a penalty term to the\n","# loss function that the model is trying to optimize.\n","\n","    svm = LinearSVC(maxIter=10, regParam=0.1, featuresCol='features', labelCol='label')\n","\n","    svm_model = svm.fit(train_data_df)\n","    prediction_df = svm_model.transform(test_data_df)\n","\n","# Cross Verify Prediction Results\n","\n","    print(\"Predictions are prediction_df...\")\n","    prediction_df.show(5, False, vertical=True)\n","\n","#\n","#     ##################### Important : Explain DAG : Directed Asyclic Graph\n","#     #  DAG is a directed graph that contains no cycles. This means that it is a graph where\n","#     #  you can only move from one vertex to another in one direction, and you cannot get back\n","#     #  to the starting point by following the edges. DAGs are used to represent many types of\n","#     #  data and processes in computer science, including data dependencies, task scheduling, and more.\n","#     ####################\n","\n","# Prediction Accuracy of Model\n","#     # For multiclass classification problems in PySpark, you can use other\n","#     # evaluation metrics such as 'accuracy', 'weightedPrecision', 'weightedRecall', 'f1',\n","#     # 'weightedFMeasure', 'microAvgFMeasure', 'macroAvgFMeasure', 'weightedFalsePositiveRate',\n","#     # 'weightedTruePositiveRate'\n","#\n","#     # Important : Here MulticlassClassificationEvaluator is used because\n","#     #           : BinaryClassificationEvaluator shows 0.0 percent accuracy where a\n","#     #           : the scatter plot gives correct accuracy. So change it to\n","#     #           : MulticlassClassificationEvaluator, as follows:-\n","#\n","    evaluator = MulticlassClassificationEvaluator() \\\n","        .setLabelCol(\"label\") \\\n","        .setPredictionCol(\"prediction\") \\\n","        .setMetricName(\"accuracy\")\n","\n","    prediction_accuracy = evaluator.evaluate(prediction_df)\n","\n","    print(\"Prediction Accuracy of SVM Model =\", prediction_accuracy * 100)\n","\n","#     # Plot a Graph\n","\n","    labels = prediction_df.select(\"label\").toPandas()\n","    predictions = prediction_df.select(\"prediction\").toPandas()\n","\n","    plt.xlabel(\"Predictions\")\n","\n","    plt.ylabel(\"label\")\n","\n","    plt.scatter(predictions, labels)\n","    plt.show()\n","\n","#     # Save Your Model as follows:-\n","\n","    print()\n","    print()\n","\n","    svm_model.write().overwrite() \\\n","        .save(\"/content/drive/MyDrive/my_model_SVM\")\n","    print(\"Your Model is saved in spark warehouse...as email_spam_model_rf\")\n","\n","    print(\"feature_df Schema\")\n","    feature_df.printSchema()\n","\n","# Cross Verification : Check the number of features in the training and test data\n","\n","    print(\"Number of features in training data:\", svm_model.numFeatures)\n","    print(\"Number of features in test data:\", test_data_df.select(\"features\").take(1)[0].features.size)\n","\n","\n","    display(HTML(\"<h3 style='color:magenta; text-align:left;'>Transformation Finished...</h3>\"))\n","    main_function()\n","\n","def load_model_and_predict(event):\n","    print()\n","    clear_result_grid()\n","    display(HTML(\"<h3 style='color:orange; text-align:left;'>Prediction Process started, Please wait !!!</h3>\"))\n","\n","    loaded_model = LinearSVCModel.load(\"/content/drive/MyDrive/my_model_SVM\")\n","\n","# Open the file in the original encoding (e.g. ANSI)\n","\n","    user_df_csv = my_spark.read \\\n","        .format(\"csv\") \\\n","        .option(\"header\", True) \\\n","        .option(\"inferSchema\", True) \\\n","        .option(\"quote\", \"\\\"\") \\\n","        .load(\"/content/drive/MyDrive/user_emails.csv\")\n","\n","# Merge or Append the actual dataset 'spam_naipu.csv' with user inputs using union command\n","# This operation is performed because in the saved model there are 8000 feature columns\n","# where as in the end user input emails total number of feature columns will be less than\n","# 8000. In this case pyspark throws 'stage failure errors'.\n","\n","    original_df = my_spark.read \\\n","        .format(\"csv\") \\\n","        .option(\"header\", True) \\\n","        .option(\"inferSchema\", True) \\\n","        .option(\"quote\", \"\\\"\") \\\n","        .load(\"/content/drive/MyDrive/spam_naipu.csv\")\n","\n","# Combine original dataframe with user_df, 'union' is used for this\n","\n","\n","    user_df_csv = original_df.union(user_df_csv)\n","\n","\n","# # Remove zero length documents\n","\n","    add_length_col_user = user_df_csv.withColumn(\"text_length\",\n","                                             length(user_df_csv[\"text\"]))\n","\n","    add_length_col_user.createOrReplaceTempView(\"drop_zero_docs\")\n","\n","    remove_0_docs = my_spark.sql(\"SELECT * FROM drop_zero_docs where text_length > 0\")\n","\n","    remove_0_docs.createOrReplaceTempView(\"clean_class_view\")\n","\n","    add_length_new_user = my_spark.sql(\"SELECT * FROM clean_class_view WHERE class = 'ham' OR class = 'spam'\")\n","\n","    # print(\"Statistics of remove_0_lenthg documents\")\n","    # remove_0_docs.describe().show()\n","\n","# Split each words and Tokenize it\n","\n","    tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"text_tokenized\")\n","    tokenized_words_df_user = tokenizer.transform(remove_0_docs)\n","\n","    stop_words_grouping = StopWordsRemover(inputCol=\"text_tokenized\", outputCol=\"text_with_no_stopwords\")\n","\n","    stop_words_removed_user = stop_words_grouping.transform(tokenized_words_df_user)\n","\n","    # print(\"stop_words_removed_user\")\n","    # stop_words_removed_user.show(2, False, vertical=True)\n","\n","    count_vector_user= CountVectorizer(inputCol=\"text_tokenized\",\n","                                       outputCol=\"count_vectors\", vocabSize=8000)\n","\n","#  # fit the CountVectorizer to the data\n","\n","    my_model_user = count_vector_user.fit(stop_words_removed_user)\n","\n","# transform the data using the CountVectorizer\n","\n","    result_user = my_model_user.transform(stop_words_removed_user)\n","\n","    tf_idf_user = IDF(inputCol=\"count_vectors\", outputCol=\"tf_idf\")\n","    tf_idf_model = tf_idf_user.fit(result_user)\n","\n","#  # Transform tf_idf_model with original data\n","\n","    tf_idf_transformed_user = tf_idf_model.transform(result_user)\n","\n","    inputs = [\"tf_idf\", \"text_length\"]\n","    output = \"features\"\n","\n","    ham_spam_assembler = VectorAssembler(inputCols=inputs, outputCol=output)\n","    feature_df_user = ham_spam_assembler.transform(tf_idf_transformed_user)\n","\n","# Cross Verify - Important\n","# Total number of features in original dataset is 8001, including text_length\n","# This should tally with the user_emails dataset, otherwise 'Stage failure' and\n","# 'index out of bounds or range error will display\n","\n","##### Start From here\n","\n","# Count the number of non-zero elements in the \"features\" column\n","############\n","#     # Select the \"features\" column from the DataFrame\n","#     existing_features = feature_df_user.select(\"features\")\n","#     print(\"existing features columns in feature_df_user are...\")\n","#     existing_features.show(5)\n","#     missing_features = [f'feature{i}' for i in range(num_features, 8000)]\n","#     #\n","#     # zeros = [0 for _ in range(len(feature_df_user.select(\"features\").first()))]\n","#     #\n","#     feature_df_user = concat(feature_df_user[\"features\"], * missing_features)\n","#     print(\"feature_df_user Schema\")\n","#     feature_df_user.printSchema()\n","# #    feature_df_user = feature_df_user.select(\"features\")\n","#\n","# # Fill in any missing values with zeros\n","# # Replace null or missing values with zeros\n","#\n","#     feature_df_user = feature_df_user.select([when(feature_df_user[col].isNull(), 0.0).otherwise(feature_df_user[col]).alias(col) for col in feature_df_user.columns])\n","\n","# Cross Verify - Important\n","# Total number of features in original dataset is 8001, including text_length\n","# This should tally with the user_emails dataset, otherwise 'Stage failure' and\n","# 'index out of bounds or range error will display\n","\n","    # print(\"Number of features in user test data:\",\n","    #       feature_df_user.select(\"features\").take(1)[0].features.size)\n","\n","############ Actual prediction using user data Start from here\n","\n","    prediction_df_user = loaded_model.transform(feature_df_user)\n","\n","# Sometimes ALS algorithm generates null values, so drop those null values\n","\n","    prediction_df_user = prediction_df_user.dropna()\n","#\n","\n","#\n","#     prediction_df_user.show(5, False, vertical=True)\n","#\n","# Create a new column that contains the first 3 characters of the text column(test)\n","# The user can input actual email as follows :-\n","#\n","# aks stands for short form of akshaya\n","#\n","# aks,test message typed today,,,\n","# aks,test taken from seb gmail spam You've won! please confirm receipt Now Get your\n","# aks,test Ozwin -Bonus? Congratulations! You are the chosen one!e!,,,\n","# aks,test message typed again manually,,,\n","#\n","    prediction_df_user  = prediction_df_user.withColumn('first_four_chars', substring('text', 1, 4))\n","\n","#\n","# Re-Convert labels back to ham or spam (For prediction ham=0.0, and spam=1.0)\n","\n","    prediction_df_user_converted = prediction_df_user\\\n","         .withColumn(\"prediction_converted\", when(prediction_df_user.prediction == 0.0, \"ham\")\n","                                       .when(prediction_df_user.prediction  == 1.0, \"spam\")\n","                                       .otherwise('Error'))\n","\n","# Display prediction results of user mails start with 'test'\n","#\n","    prediction_df_user_converted.createOrReplaceTempView(\"last_view\")\n","\n","    my_spark.sql(\"SELECT text, prediction,prediction_converted FROM \"\n","                  \"last_view WHERE first_four_chars = 'test'\")\\\n","         .show(20, False, vertical=True)\n","    display(HTML(\"<h3 style='color:magenta; text-align:left;'>Prediction Finished...</h3>\"))\n","    main_function()\n","# The following function predict single emails\n","\n","def input_email_and_predict(event):\n","    print()\n","    clear_result_grid()\n","    filename = '/content/drive/MyDrive/user_input_mail.csv'\n","\n","    headers = ['class','text']\n","\n","    css = \"\"\"\n","      <style>\n","          .red-text .widget-label {\n","              color: red !important;\n","              font-size: 18px;\n","          }\n","      </style>\n","      \"\"\"\n","    display(HTML(css))\n","    print()\n","    print()\n","\n","# Display Main Heading\n","\n","    display(HTML(\"<h2 style='color:green; text-align:center;'>Final Prediction using new Emails</h2>\"))\n","    print()\n","\n","# Actual GUI descriptions for each required entry parameters declared here\n","\n","    user_class = widgets.Dropdown(options = ['xxx,'],\n","                                  description ='Input not allowed here>>>>>>>>',\n","                                  style = {'description_width': 'initial'},\n","                                  layout=widgets.Layout(width='350px'))\n","\n","    user_mail = widgets.Textarea(value='test',description='Copy and Paste your email>>>>',\n","                                    style={'description_width': 'initial'},\n","                                    layout=widgets.Layout(width='1000px',height='200px'))\n","\n","# New description below the Textarea\n","    # new_description = widgets.Label(\"Note: The word 'test' cannot be removed from the beginning.\"\n","    # \"Paste your Email after 'test'\")\n","\n","    new_description = widgets.HTML(\n","    \"<p style='color: BLUE;'>Note: The word 'test' cannot be removed from the beginning. \"\n","    \"Paste your Email after 'test'</p>\"\n","     )\n","# Add CSS classes declared above to the input widgets\n","\n","    user_class.add_class('red-text')\n","    user_class.add_class('custom-input-box')\n","    user_class_container = widgets.HBox(children=[user_class])\n","\n","\n","    user_mail.add_class('red-text')\n","    user_mail.add_class('custom-input-box')\n","\n","    user_mail_container = widgets.VBox(children=[user_mail,new_description])\n","\n","# Create a Save or Submit button\n","\n","    submit_button = widgets.Button(description='Save', style={'button_color': 'yellow'})\n","\n","# Set the size of the button using Layout object\n","\n","    button_layout = widgets.Layout(width='200px', height='50px', grid_area='center')\n","    submit_button.layout = button_layout\n","\n","# Create Predict button\n","\n","    button_predict = widgets.Button(description='Predict', style={'button_color': 'yellow'})\n","    button_predict.layout= button_layout\n","\n","    buttons_container = widgets.HBox(children=[submit_button, button_predict])\n","\n","#    display(user_class_container)\n","\n","    display(user_mail_container)\n","\n","# Apply CSS style to center the buttons\n","\n","    centered_style = \"\"\"\n","    display: flex;\n","    justify-content: center;\n","    \"\"\"\n","    buttons_container.add_class('centered-buttons')\n","    display(HTML(f'<style>.centered-buttons{{{centered_style}}}</style>'))\n","    display(buttons_container)\n","    def on_button_clicked(submit_button):\n","# Assign values from text boxes to variables\n","      entered_class = user_class.value\n","      entered_mail = user_mail.value\n","# Save your data as csv file 'user_input_house.csv'\n","      with open(filename, 'w', newline='') as f:\n","              writer = csv.writer(f)\n","              writer.writerow(headers)\n","              writer.writerow([entered_class, entered_mail])\n","# $$ modify print statement\n","              print('Data has been written to', filename)\n","\n","    submit_button.on_click(on_button_clicked)\n","    button_predict.on_click(predict_function)\n","    return True\n","def predict_function(parameter):\n","\n","    # display(HTML(\"<h3 style='color:green; text-align:center;'>Prediction Process Single Mail started..may take few seconds !!!</h3>\"))\n","    # print()\n","# Transfer Control from GUI to Colab Notebook function\n","    load_model_single_email()\n","    return\n","\n","def load_model_single_email():\n","    print()\n","    clear_result_grid()\n","    display(HTML(\"<h3 style='color:orange; text-align:left;'>Prediction Process single \"\n","      \"Mail started, Please wait !!!</h3>\"))\n","\n","    loaded_model = LinearSVCModel.load(\"/content/drive/MyDrive/my_model_SVM\")\n","\n","# Open the file in the original encoding (e.g. ANSI)\n","\n","    user_df_csv = my_spark.read \\\n","        .format(\"csv\") \\\n","        .option(\"header\", True) \\\n","        .option(\"inferSchema\", True) \\\n","        .option(\"quote\", \"\\\"\") \\\n","        .load(\"/content/drive/MyDrive/user_input_mail.csv\")\n","\n","# Merge or Append the actual dataset 'spam_naipu.csv' with user inputs using union command\n","# This operation is performed because in the saved model there are 8000 feature columns\n","# where as in the end user input emails total number of feature columns will be less than\n","# 8000. In this case pyspark throws 'stage failure errors'.\n","\n","    original_df = my_spark.read \\\n","        .format(\"csv\") \\\n","        .option(\"header\", True) \\\n","        .option(\"inferSchema\", True) \\\n","        .option(\"quote\", \"\\\"\") \\\n","        .load(\"/content/drive/MyDrive/spam_naipu.csv\")\n","\n","# Combine original dataframe with user_df, 'union' is used for this\n","\n","    user_df_csv = original_df.union(user_df_csv)\n","\n","# # Remove zero length documents\n","\n","    add_length_col_user = user_df_csv.withColumn(\"text_length\",\n","                                             length(user_df_csv[\"text\"]))\n","\n","    add_length_col_user.createOrReplaceTempView(\"drop_zero_docs\")\n","\n","    remove_0_docs = my_spark.sql(\"SELECT * FROM drop_zero_docs where text_length > 0\")\n","\n","    remove_0_docs.createOrReplaceTempView(\"clean_class_view\")\n","\n","    add_length_new_user = my_spark.sql(\"SELECT * FROM clean_class_view WHERE class = 'ham' OR class = 'spam'\")\n","\n","    # print(\"Statistics of remove_0_lenthg documents\")\n","    # remove_0_docs.describe().show()\n","\n","# Split each words and Tokenize it\n","\n","    tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"text_tokenized\")\n","    tokenized_words_df_user = tokenizer.transform(remove_0_docs)\n","\n","    stop_words_grouping = StopWordsRemover(inputCol=\"text_tokenized\", outputCol=\"text_with_no_stopwords\")\n","\n","    stop_words_removed_user = stop_words_grouping.transform(tokenized_words_df_user)\n","\n","    # print(\"stop_words_removed_user\")\n","    # stop_words_removed_user.show(2, False, vertical=True)\n","\n","    count_vector_user= CountVectorizer(inputCol=\"text_tokenized\",\n","                                       outputCol=\"count_vectors\", vocabSize=8000)\n","\n","#  # fit the CountVectorizer to the data\n","\n","    my_model_user = count_vector_user.fit(stop_words_removed_user)\n","\n","# transform the data using the CountVectorizer\n","\n","    result_user = my_model_user.transform(stop_words_removed_user)\n","\n","    tf_idf_user = IDF(inputCol=\"count_vectors\", outputCol=\"tf_idf\")\n","    tf_idf_model = tf_idf_user.fit(result_user)\n","\n","#  # Transform tf_idf_model with original data\n","\n","    tf_idf_transformed_user = tf_idf_model.transform(result_user)\n","\n","    inputs = [\"tf_idf\", \"text_length\"]\n","    output = \"features\"\n","\n","    ham_spam_assembler = VectorAssembler(inputCols=inputs, outputCol=output)\n","    feature_df_user = ham_spam_assembler.transform(tf_idf_transformed_user)\n","\n","# Cross Verify - Important\n","# Total number of features in original dataset is 8001, including text_length\n","# This should tally with the user_emails dataset, otherwise 'Stage failure' and\n","# 'index out of bounds or range error will display\n","\n","##### Start From here\n","\n","# Count the number of non-zero elements in the \"features\" column\n","############\n","#     # Select the \"features\" column from the DataFrame\n","#     existing_features = feature_df_user.select(\"features\")\n","#     print(\"existing features columns in feature_df_user are...\")\n","#     existing_features.show(5)\n","#     missing_features = [f'feature{i}' for i in range(num_features, 8000)]\n","#     #\n","#     # zeros = [0 for _ in range(len(feature_df_user.select(\"features\").first()))]\n","#     #\n","#     feature_df_user = concat(feature_df_user[\"features\"], * missing_features)\n","#     print(\"feature_df_user Schema\")\n","#     feature_df_user.printSchema()\n","# #    feature_df_user = feature_df_user.select(\"features\")\n","#\n","# # Fill in any missing values with zeros\n","# # Replace null or missing values with zeros\n","#\n","#     feature_df_user = feature_df_user.select([when(feature_df_user[col].isNull(), 0.0).otherwise(feature_df_user[col]).alias(col) for col in feature_df_user.columns])\n","\n","# Cross Verify - Important\n","# Total number of features in original dataset is 8001, including text_length\n","# This should tally with the user_emails dataset, otherwise 'Stage failure' and\n","# 'index out of bounds or range error will display\n","\n","    # print(\"Number of features in user test data:\",\n","    #       feature_df_user.select(\"features\").take(1)[0].features.size)\n","\n","############ Actual prediction using user data Start from here\n","\n","    prediction_df_user = loaded_model.transform(feature_df_user)\n","\n","# Sometimes ALS algorithm generates null values, so drop those null values\n","\n","    prediction_df_user = prediction_df_user.dropna()\n","#\n","\n","    prediction_df_user  = prediction_df_user.withColumn('first_four_chars', substring('text', 1, 4))\n","\n","# Re-Convert labels back to ham or spam (For prediction ham=0.0, and spam=1.0)\n","\n","    prediction_df_user_converted = prediction_df_user\\\n","         .withColumn(\"prediction_converted\", when(prediction_df_user.prediction == 0.0, \"ham\")\n","                                       .when(prediction_df_user.prediction  == 1.0, \"spam\")\n","                                       .otherwise('Error'))\n","\n","# Display prediction results of user mails start with 'test'\n","\n","    prediction_df_user_converted.createOrReplaceTempView(\"last_view\")\n","\n","    # my_spark.sql(\"SELECT text, prediction,prediction_converted FROM \"\n","    #               \"last_view WHERE first_four_chars = 'test'\")\\\n","    #      .show(20, False, vertical=True)\n","\n","# By using triple quotes(\"\"\"), you can write the entire SQL query as a single string,\n","# even if it spans multiple lines\n","\n","    html_table = my_spark.sql(\"\"\"\n","    SELECT prediction_converted,\n","           CASE WHEN prediction_converted = 'ham'\n","           THEN '<h2 style=\"font-size: 24px;color:green;\">Not Spam mail !!!</h2>'\n","                WHEN prediction_converted = 'spam'\n","           THEN '<h2 style=\"font-size: 24px;color:red;\">SPAM mail !!!</h2>'\n","           ELSE prediction_converted\n","                END AS prediction_converted_html\n","    FROM last_view\n","    WHERE first_four_chars = 'test' AND (prediction_converted = 'ham' OR prediction_converted = 'spam')\n","    \"\"\")\n","\n","# Convert the result to Pandas DataFrame\n","    pandas_df = html_table.toPandas()\n","\n","# Generate HTML content\n","    html_content = pandas_df[[\"prediction_converted_html\"]].to_html(index=False, header=False, escape=False)\n","\n","# Display HTML content\n","    display(HTML(html_content))\n","    print()\n","    display(HTML(\"<h4 style='color:black; text-align:left;'>Re-run Code cell again for next Prediction</h3>\"))\n","\n","def main_function():\n","\n","# Set focus to result grid\n","  set_focus()\n","  print()\n","  display(HTML(\"<h3 style='color:magenta; text-align:center;'>Main Menu - Email ham or spam Prediction</h3>\"))\n","  print()\n","# Create your buttons for each events\n","  load_data_button = widgets.Button(description=\"1. Load Data\")\n","  eda_analysis_button = widgets.Button(description=\"2. EDA Analysis\")\n","  pre_process_button = widgets.Button(description=\"3. Pre Process Data\")\n","  transform_build_button = widgets.Button(description=\"4. Transform & Build\")\n","  load_model_and_predict_button = widgets.Button(description=\"5. Prediction(Bulk Mails)\")\n","  single_mail_predict_button = widgets.Button(description=\"6. Prediction(Single Mail)\")\n","\n","# Assign python functions as event handlers\n","  load_data_button.on_click(load_data)\n","  eda_analysis_button.on_click(EDA_start)\n","  pre_process_button.on_click(pre_process_data)\n","  transform_build_button.on_click(transformation)\n","  load_model_and_predict_button.on_click(load_model_and_predict)\n","  single_mail_predict_button.on_click(input_email_and_predict)\n","\n","# Define custom CSS to adjust button dimensions, backcolor and textcolor\n","\n","  custom_css = \"\"\"\n","  <style>\n","  .custom-button {\n","      width: 170px;  /* Change this value for width */\n","      height: 50px;  /* Change this value for height */\n","      color: blue; /* Text color */\n","      background-color: lightgreen;  /* Background color */\n","      border: 2px solid black;\n","  }\n","/* Add a hover effect */\n","  .custom-button:hover {\n","      background-color: white; /* Change background color on hover */\n","      }\n","  </style>\n","  \"\"\"\n","\n","  # Apply CSS classes to buttons\n","\n","  load_data_button.add_class(\"custom-button\")\n","  eda_analysis_button.add_class(\"custom-button\")\n","  pre_process_button.add_class(\"custom-button\")\n","  transform_build_button.add_class(\"custom-button\")\n","  load_model_and_predict_button.add_class(\"custom-button\")\n","  single_mail_predict_button.add_class(\"custom-button\")\n","\n","# Display the custom CSS and buttons\n","# is ensuring that the CSS rules are applied to the buttons' appearance when\n","# they are displayed in the notebook.\n","\n","  display(HTML(custom_css))\n","\n","# Arrange buttons horizontally\n","\n","  buttons_horizontal_layout_1 = widgets.HBox([load_data_button, eda_analysis_button, pre_process_button])\n","  buttons_horizontal_layout_2 = widgets.HBox([transform_build_button, load_model_and_predict_button, single_mail_predict_button])\n","\n","  # Center the buttons using a centered container in result grid\n","\n","  centered_layout = widgets.Layout(justify_content='center')\n","\n","  # Arrange buttons horizontally\n","\n","  centered_container_1 = widgets.Box([buttons_horizontal_layout_1], layout=centered_layout)\n","  centered_container_2 = widgets.Box([buttons_horizontal_layout_2], layout=centered_layout)\n","\n","  # Display the centered buttons\n","\n","  display(centered_container_1)\n","  print()\n","  display(centered_container_2)\n","  print()\n","# Declare a main function as follows to invoke all other function\n","\n","if __name__ == \"__main__\":\n","\n","  main_function()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":237,"referenced_widgets":["e9dd91bd67a64fcca2950d45ce8a036a","bafbab0d8e3b4095a91ffb5b73e060b6","08191eea271d418f8c49f9b8cda9e9e1","5065bfeacd6641c784e9fd93143f9303","d4cfa0c4db234014a0a6a105fff1c0c4","f995e8b55dfd45ca97a2658dd0b70d46","4214de2be88a433a9ecdc3bac651258c","f30dfa66ee5c402092189d6240394195","9a42f6241baf4601a55d340198d3c3e3","2e68bccd85cf4cc48e0fd5757797b467","a1d45c4171714ed4ba2e580dc3de2d76","06c93b7633554e65ac4cb503e6953fbb","3d6e8185bc7a42cb93fd2e2c06093ee6","54d3069ea7e142ab9d5925f924916e31","8fd3e07129524dd39740aa2b9ccd6011","c9fea9baf2f846a392a5736127ccdc1e","aeba101823e642f4b87a2b7ade7a4480","704bd9b1b00a41fd9358c39ee14b733d","e9d432fd493a479abe87a486e45feb13","3764bebee3bc41e39484267bc4e5cf20","59a103bc21fb43ccaea852ac4ced1675","3fd06e4a16384a2da18f1e5952ce1324","f4b403023c654be59547e87e0dece506","e20de2acffd1403090ea41fc19f1d9a1","f2d245be25274337bcabcc7ec9560d75"]},"id":"ODlmHBLAiFTO","executionInfo":{"status":"ok","timestamp":1700684983976,"user_tz":-330,"elapsed":1001,"user":{"displayName":"TANUSHREE MONDAL HAZRA","userId":"12079523468331201661"}},"outputId":"75357622-e3bf-4db6-ccf1-70ef42df56bc"},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","        document.getElementById('output-area').scrollIntoView({behavior: 'smooth'});\n","    "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","        document.getElementById('output-area').scrollIntoView({behavior: 'smooth'});\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<h3 style='color:magenta; text-align:center;'>Main Menu - Email ham or spam Prediction</h3>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","  <style>\n","  .custom-button {\n","      width: 170px;  /* Change this value for width */\n","      height: 50px;  /* Change this value for height */\n","      color: blue; /* Text color */\n","      background-color: lightgreen;  /* Background color */\n","      border: 2px solid black;\n","  }\n","/* Add a hover effect */\n","  .custom-button:hover {\n","      background-color: white; /* Change background color on hover */\n","      }\n","  </style>\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Box(children=(HBox(children=(Button(description='1. Load Data', style=ButtonStyle(), _dom_classes=('custom-but…"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9dd91bd67a64fcca2950d45ce8a036a"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n"]},{"output_type":"display_data","data":{"text/plain":["Box(children=(HBox(children=(Button(description='4. Transform & Build', style=ButtonStyle(), _dom_classes=('cu…"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54d3069ea7e142ab9d5925f924916e31"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n"]}]}]}